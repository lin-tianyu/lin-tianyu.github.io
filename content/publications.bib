@inproceedings{lin2025are,
  title={Are Pixel-Wise Metrics Reliable for Computerized Tomography Reconstruction?},
  author={Tianyu Lin and Xinran Li and Chuntung Zhuang and Qi Chen and Yuanhao Cai and Kai Ding and Alan Yuille and Zongwei Zhou*},
  booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025), RSNA 2025 Oral},
  year={2025},
  url={https://openreview.net/forum?id=L8pbQy0SsG},
preview={lin2025are.jpg},
description={It's time to evaluate anatomical completeness in CT reconstruction!},
project={},
paper={https://openreview.net/forum?id=L8pbQy0SsG},
code={https://github.com/MrGiovanni/CARE},
arxiv={2506.02093},
abstract={Widely adopted evaluation metrics for sparse-view CT reconstruction, such as Structural Similarity Index Measure and Peak Signal-to-Noise Ratio, prioritize pixel-wise fidelity but often fail to capture the completeness of critical anatomical structures, particularly small or thin regions that are easily missed. To address this limitation, we propose a suite of novel anatomy-aware evaluation metrics designed to assess structural completeness across anatomical structures, including large organs, small organs, intestines, and vessels. Building on these metrics, we introduce CARE, a Completeness-Aware Reconstruction Enhancement framework that incorporates structural penalties during training to encourage anatomical preservation of significant structures. CARE is model-agnostic and can be seamlessly integrated into analytical, implicit, and generative methods. When applied to these methods, CARE substantially improves structural completeness in CT reconstructions, achieving up to 32% improvement for large organs, 22% for small organs, 40% for intestines, and 36% for vessels.},
selected={true},
}

@inproceedings{lin2024stable,
  title={Stable diffusion segmentation for biomedical images with single-step reverse process},
  author={Lin, Tianyu and Chen, Zhiguang and Yan, Zhonghao and Yu, Weijiang* and Zheng, Fudan*},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2024)},
  pages={656--666},
  year={2024},
  organization={Springer},

  preview={lin2024stable.jpg},
  description={A single-step latent space diffusion segmentation model},
  project={https://lin-tianyu.github.io/Stable-Diffusion-Seg/},
  paper={https://link.springer.com/chapter/10.1007/978-3-031-72111-3_62},
  code={https://github.com/lin-tianyu/Stable-Diffusion-Seg},
  arxiv={2406.18361},
  abstract={Diffusion models have demonstrated their effectiveness across various generative tasks. However, when applied to medical image segmentation, these models encounter several challenges, including significant resource and time requirements. They also necessitate a multi-step reverse process and multiple samples to produce reliable predictions. To address these challenges, we introduce the first latent diffusion segmentation model, named SDSeg, built upon stable diffusion (SD). SDSeg incorporates a straightforward latent estimation strategy to facilitate a single-step reverse process and utilizes latent fusion concatenation to remove the necessity for multiple samples. Extensive experiments indicate that SDSeg surpasses existing state-of-the-art methods on five benchmark datasets featuring diverse imaging modalities. Remarkably, SDSeg is capable of generating stable predictions with a solitary reverse step and sample, epitomizing the modelâ€™s stability as implied by its name. The code is available at https://github.com/lin-tianyu/Stable-Diffusion-Seg.},
  selected={true},
}

@misc{li2025expectationmaximizationenginescalablemedical,
      title={Expectation-Maximization as the Engine of Scalable Medical Intelligence}, 
      author={Wenxuan Li and Pedro R. A. S. Bassi and Tianyu Lin and Yu-Cheng Chou and Jakob Wasserthal and Xinze Zhou and Qi Chen and Fabian Isensee and Yannick Kirchhoff and Maximilian Rokuss and Saikat Roy and Constantin Ulrich and Klaus Maier-Hein and Szymon PÅ‚otka and Xiaoxi Chen and Kang Wang and Yang Yang and Daguang Xu and Kai Ding and Yucheng Tang and Alan L. Yuille and Zongwei Zhou*},
      year={2025},
      journal={arXiv preprint arXiv:2501.03410},
      eprint={2501.03410},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.03410}, 
  preview={li2025expectationmaximizationenginescalablemedical.jpg},
  description={ScaleMAI: Accelerating the development of trusted datasets and ai models},
  project={https://www.zongweiz.com/dataset},
  paper={},
  code={https://github.com/MrGiovanni/ScaleMAI},
  arxiv={2501.03410},
  abstract={Large, high-quality, annotated datasets are the foundation of medical AI research, but constructing even a small, moderate-quality, annotated dataset can take years of effort from multidisciplinary teams. Although active learning can prioritize what to annotate, scaling up still requires extensive manual efforts to revise the noisy annotations. We formulate this as a missing-data problem and develop ScaleMAI, a framework that unifies data annotation and model development co-evolution through an Expectation-Maximization (EM) process. In this iterative process, the AI model automatically identifies and corrects the mistakes in annotations (Expectation), while the refined annotated data retrain the model to improve accuracy (Maximization). In addition to the classical EM algorithm, ScaleMAI brings human experts into the loop to review annotations that cannot be adequately addressed by either Expectation or Maximization step (<5%). As a result, ScaleMAI progressively creates an annotated dataset of 47,315 CT scans (4.8x larger than the largest public dataset, PanTS) including 4,163,720 per-voxel annotations for benign/malignant tumors and 88 anatomical structures. ScaleMAI iteratively trains a model that exceeds human expert performance in tumor diagnosis (+7%), and outperforms models developed from smaller, moderate-quality datasets, with statistically significant gains in tumor detection (+10%) and segmentation (+14%) on two prestigious benchmarks.},
  selected={false},
}



@inproceedings{yan2025pgp,
  title={PGP-SAM: Prototype-Guided Prompt Learning for Efficient Few-Shot Medical Image Segmentation},
  author={Yan, Zhonghao and Yin, Zijin and Lin, Tianyu and Zeng, Xiangzhu and Liang, Kongming* and Ma, Zhanyu},
  booktitle={2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)},
  pages={1--5},
  year={2025},
  organization={IEEE},

preview={yan2025pgp.jpg},
description={},
project={},
paper={https://ieeexplore.ieee.org/document/10980911},
code={https://github.com/PRIS-CV/PGP-SAM},
arxiv={2501.06692},
abstract={The Segment Anything Model (SAM) has demonstrated strong and versatile segmentation capabilities, along with intuitive prompt-based interactions. However, customizing SAM for medical image segmentation requires massive amounts of pixel-level annotations and precise point- or box-based prompt designs. To address these challenges, we introduce PGP-SAM, a novel prototype-based few-shot tuning approach that uses limited samples to replace tedious manual prompts. Our key idea is to leverage inter- and intra-class prototypes to capture class-specific knowledge and relationships. We propose two main components: (1) a plug-and-play contextual modulation module that integrates multi-scale information, and (2) a class-guided cross-attention mechanism that fuses prototypes and features for automatic prompt generation. Experiments on a public multi-organ dataset and a private ventricle dataset demonstrate that PGP-SAM achieves superior mean Dice scores compared with existing prompt-free SAM variants, while using only 10% of the 2D slices.},
selected={false},
}

@article{lu2025priority,
  title = {A priority-guided contrastive network for delineating vascular layers in arterial ultrasound},
  journal = {Expert Systems with Applications},
  volume = {272},
  pages = {126695},
  year = {2025},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2025.126695},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417425003173},
  author = {Minhua Lu# and Tianyu Lin# and Weiyuan Lin* and Zhaohui Li and Zhifan Gao},
  keywords = {Carotid intima-media, Semi-supervised segmentation, Pseudo label, Contrastive learning, Class imbalance},
preview={lu2025priority.jpg},
description={},
project={},
paper={https://www.sciencedirect.com/science/article/pii/S0957417425003173},
code={},
arxiv={},
abstract={Arterial thickness is a crucial metric for evaluating the risk of atherosclerosis, often measured from ultrasound images. We propose a new learning framework, priority-guided contrastive network, for accurate delineation of artery. Our network can effectively balance data distribution and ensure that critical boundary details are captured with reduced errors. Specifically, the network prioritizes underrepresented pixels of arterial regions by dynamic priority induction and extract anchor pixels from ambiguous pseudo-labels via contrastive learning. These pixels are converted into meaningful supervision signals, so our network can focus on critical yet scarce regions and capture anatomical continuity in the artery. Experimental evaluations on two datasets validate the effectiveness of our method, achieving Dice similarity coefficients of 0.815 and 0.726 under extreme low-label partition protocols. Our approach outperforms multiple detection methods of common computer vision and artery. Our method shows potential for application beyond the particular training data used in the study. The accuracy afforded by the framework would allow it to be incorporated into the clinical workflow for improved detection of artery.},
selected={false},
}

@inproceedings{chen2025emssd,
  author={Chen, Ruiyue and Zhang, Xin* and Lin, Tianyu and Yu, Sijin},
  booktitle={2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI 2025)}, 
  title={EMSSD: Two-Stage Model Enhancing Medical Image Segmentation Based on Stable Diffusion}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  keywords={Image segmentation;Solid modeling;Three-dimensional displays;Image synthesis;Computational modeling;Biological system modeling;Noise reduction;Diffusion models;Robustness;Biomedical imaging;Medical Image Segmentation;Stable Diffusion;Latent Diffusion Model;Boundary Prior},
  doi={10.1109/ISBI60581.2025.10981045},
preview={chen2025emssd.jpg},
description={},
project={},
paper={https://ieeexplore.ieee.org/document/10981045},
code={},
arxiv={},
abstract={Denoising Diffusion Probabilistic Models have recently demonstrated remarkable in various image generation tasks. With robust nonlinear modeling capacity and superior generalization performance, denoising diffusion models are being progressively applied in medical image segmentation. However, popular diffusion models trained with 3D patch-based or 2D slice-wise inputs may lack a global anatomical perception of medical tissues and performing operations in pixel space incurs substantial computational overhead. In this study, we propose a two-stage framework, called Enhancing Medical image Segmentation based on Stable Diffusion (EMSSD), to elevate segmentation accuracy. A 3D segmentation model is pretrained in the first stage to obtain coarse segmentation maps which along with the input images are jointly encoded as constraints into latent diffusion model to direct model's attention to the target region in the second stage. To sharpen the guidance of fuzzy boundary prior from coarse segmentation results while adequately leveraging the medical images, we design a Condition Fusion Module (CFM) to aggregate both conditions. We also introduce a single-step reverse process to replace multi-step to optimize time efficiency. Experiments conducted on two datasets have substantiated the effective-ness and generalization of our approach.},
selected={false},
}




@misc{deng2025evaluatinglargelanguagemodels,
      title={Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines}, 
      author={Guifeng Deng and Shuyin Rao and Tianyu Lin and Anlu Dai and Pan Wang and Junyi Xie and Haidong Song and Ke Zhao and Dongwu Xu and Zhengdong Cheng and Tao Li* and Haiteng Jiang*},
      year={2025},
      eprint={2506.01329},
      journal={arXiv preprint arXiv:2506.01329},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.01329}, 
preview={},
description={},
project={},
paper={},
code={},
arxiv={2506.01329},
abstract={Psychological support hotlines serve as critical lifelines for crisis intervention but encounter significant challenges due to rising demand and limited resources. Large language models (LLMs) offer potential support in crisis assessments, yet their effectiveness in emotionally sensitive, real-world clinical settings remains underexplored. We introduce PsyCrisisBench, a comprehensive benchmark of 540 annotated transcripts from the Hangzhou Psychological Assistance Hotline, assessing four key tasks: mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment. 64 LLMs across 15 model families (including closed-source such as GPT, Claude, Gemini and open-source such as Llama, Qwen, DeepSeek) were evaluated using zero-shot, few-shot, and fine-tuning paradigms. LLMs showed strong results in suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779), and risk assessment (F1=0.907), with notable gains from few-shot prompting and fine-tuning. Compared to trained human operators, LLMs achieved comparable or superior performance on suicide plan identification and risk assessment, while humans retained advantages on mood status recognition and suicidal ideation detection. Mood status recognition remained challenging (max F1=0.709), likely due to missing vocal cues and semantic ambiguity. Notably, a fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) outperformed larger models on mood and suicidal ideation tasks. LLMs demonstrate performance broadly comparable to trained human operators in text-based crisis assessment, with complementary strengths across task types. PsyCrisisBench provides a robust, real-world evaluation framework to guide future model development and ethical deployment in clinical mental health.},
selected={false},
}


@inproceedings{li2025pants,
  title={Pan{TS}: The Pancreatic Tumor Segmentation Dataset},
  author={Wenxuan Li# and Xinze Zhou# and Qi Chen# and Tianyu Lin and Pedro R. A. S. Bassi and Xiaoxi Chen and Chen Ye and Zheren Zhu and Kai Ding and Heng Li and Kang Wang and Yang Yang and Yucheng Tang and Daguang Xu and Alan Yuille and Zongwei Zhou*},
  booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025) Datasets and Benchmarks Track},
  year={2025},
  url={https://openreview.net/forum?id=0BCUXg40r7},
preview={li2025pants.jpg},
description={},
project={https://www.zongweiz.com/dataset},
paper={https://openreview.net/forum?id=0BCUXg40r7},
code={https://github.com/MrGiovanni/PanTS},
arxiv={2507.01291},
abstract={PanTS is a large-scale, multi-institutional dataset curated to advance research in pancreatic CT analysis. It contains 36,390 CT scans from 145 medical centers, with expert-validated, voxel-wise annotations of over 993,000 anatomical structures, covering pancreatic tumors, pancreas head, body, and tail, and 24 surrounding anatomical structures such as vascular/skeletal structures and abdominal/thoracic organs. Each scan includes metadata such as patient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness, etc. AI models trained on PanTS achieve significantly better performance in pancreatic tumor detection, localization, and segmentation than those trained on existing public datasets. Our analysis indicates that these gains are directly attributable to the 16Ã— larger-scale tumor annotations and indirectly supported by the 24 additional surrounding anatomical structures. As the largest and most comprehensive resource of its kind, PanTS offers a new benchmark for developing and evaluating AI models in pancreatic CT analysis.},
selected={true},
}

@article{lin2025eyepose, 
  year     = {2025}, 
  title    = {{EyePose}: Pose-guided Saccadic Eye Movement Video Generation for Deep Learning-Based Neurologic Disease Phenotyping}, 
  author   = {Lin, Tianyu and Ryu, Jooyoung and Sreevar, Puvada and Srinivasaraga, Rahul and Satavlekar, Riya and Kim, Susan and Soley, Nidhi and Yan, Yujie and Vatsaraj, Ishan and Harris, Carl and Rahman, Aimon and Patel, Vishal and Greenstein, Joseph and Taylor, Casey and Green, Kemar*}, 
  doi      = {10.21203/rs.3.rs-6995265/v1}, 
  journal  = {Under review at npj Digital Medicine},
preview={lin2025eyepose.jpg},
description={Building a saccade screening tool without any private patient data},
project={https://archive.data.jhu.edu/dataset.xhtml?persistentId=doi:10.7281/T1KON8NZ},
paper={https://www.researchsquare.com/article/rs-6995265/v1},
code={},
arxiv={},
abstract={Eye movements, including saccades, are widely regarded as highly sensitive and objective biomarkers of neurophysiologic states. Detecting saccadic signatures in neurologic diseases offers a promising, rapid, portable, and non-invasive alternative to current diagnostic tools, such as brain imaging, while overcoming access limitations and cost barriers. Currently, no robust video-oculographic solutions exist for localizing brain abnormalities due to privacy concerns and the lack of large datasets required to train accurate, reliable models. In this work, we propose the first fully synthetic, patient data-free, multimodal eye movement generation pipeline for building a generalizable dataset for saccade analysis. Using this synthetic dataset, we trained a deep learning classifier to distinguish between normal and abnormal (hypometria and hypermetria) saccadic accuracies, and evaluated its performance on real-world clinical data. The model achieved an AUROC of 0.76 and sensitivity of 0.71, showing that the synthetic data has strong potential to generalize for clinical applications. This work highlights the potential of synthetic eye movement data to be used to develop screening tools for at-home and emergency room settings.},
selected={true},
}


@InProceedings{bassi2025learning,
  author="Bassi, Pedro R. A. S. and Li, Wenxuan and Chen, Jieneng and Zhu, Zheren and Lin, Tianyu and Decherchi, Sergio and Cavalli, Andrea and Wang, Kang and Yang, Yang and Yuille, Alan L. and Zhou, Zongwei*",
  editor="Gee, James C. and Alexander, Daniel C. and Hong, Jaesung and Iglesias, Juan Eugenio and Sudre, Carole H. and Venkataraman, Archana and Golland, Polina and Kim, Jong Hyo and Park, Jinah",
  title="Learning Segmentation from Radiology Reports",
  booktitle="Medical Image Computing and Computer Assisted Intervention (MICCAI 2025)",
  year="2025",
  publisher="Springer Nature Switzerland",
  address="Cham",
  pages="305--315",
  isbn="978-3-032-04971-1",
preview={bassi2025learning.jpg},
description={ðŸ† Best Paper Award Runner-up (top 2 in 1,027 papers)},
project={},
paper={https://link.springer.com/chapter/10.1007/978-3-032-04971-1_29},
code={https://github.com/MrGiovanni/R-Super},
arxiv={2507.05582},
abstract={Tumor segmentation in CT scans is key for diagnosis, surgery, and prognosis, yet segmentation masks are scarce because their creation requires time and expertise. Public abdominal CT datasets have from dozens to a couple thousand tumor masks, but hospitals have hundreds of thousands of tumor CTs with radiology reports. Thus, leveraging reports to improve segmentation is key for scaling. In this paper, we propose a report-supervision loss (R-Super) that converts radiology reports into voxel-wise supervision for tumor segmentation AI. We created a dataset with 6,718 CT-Report pairs (from the UCSF Hospital), and merged it with public CT-Mask datasets (from AbdomenAtlas 2.0). We used our R-Super to train with these masks and reports, and strongly improved tumor segmentation in internal and external validationâ€”F1 Score increased by up to 16% with respect to training with masks only. By leveraging readily available radiology reports to supplement scarce segmentation masks, R-Super strongly improves AI performance both when very few training masks are available (e.g., 50), and when many masks are available (e.g., 1.7K).},
selected={true},
}




@inproceedings{huang2025leaf,
  title={LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation},
  author={Huang, Qilin and Lin, Tianyu and Chen, Zhiguang and Zheng, Fudan*},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2025)},
  pages={384--393},
  year={2025},
  organization={Springer},
preview={huang2025leaf.jpg},
description={},
project={https://leafseg.github.io/leaf/},
paper={https://link.springer.com/chapter/10.1007/978-3-032-04978-0_37},
code={https://github.com/Pearisli/LEAF},
arxiv={2507.18214},
abstract={Leveraging the powerful capabilities of diffusion models has yielded quite effective results in medical image segmentation tasks. However, existing methods typically transfer the original training process directly without specific adjustments for segmentation tasks. Furthermore, the commonly used pre-trained diffusion models still have deficiencies in feature extraction. Based on these considerations, we propose LEAF, a medical image segmentation model grounded in latent diffusion models. During the fine-tuning process, we replace the original noise prediction pattern with a direct prediction of the segmentation map, thereby reducing the variance of segmentation results. We also employ a feature distillation method to align the hidden states of the convolutional layers with the features from a transformer-based vision encoder. Experimental results demonstrate that our method enhances the performance of the original diffusion model across multiple segmentation datasets for different disease types. Notably, our approach does not alter the model architecture, nor does it increase the number of parameters or computation during the inference phase, making it highly efficient.},
selected={false},
}

@misc{chen2025radfabricagenticaireasoning,
      title={RadFabric: Agentic AI System with Reasoning Capability for Radiology}, 
      author={Wenting Chen# and Yi Dong# and Zhaojun Ding and Yucheng Shi and Yifan Zhou and Fang Zeng and Yijun Luo and Tianyu Lin and Yihang Su and Yichen Wu and Kai Zhang and Zhen Xiang and Tianming Liu and Ninghao Liu and Lichao Sun* and Yixuan Yuan* and Xiang Li*},
      year={2025},
      eprint={2506.14142},
      journal={arXiv preprint arXiv:2506.14142},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.14142}, 
preview={chen2025radfabricagenticaireasoning.jpg},
description={},
project={https://yidong11.github.io/Towards-Multi-Modal-Agentic-AI-System-for-Chest-X-Ray/},
paper={},
code={https://github.com/yidong11/Towards-Multi-Modal-Agentic-AI-System-for-Chest-X-Ray},
arxiv={2506.14142},
abstract={Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.},
selected={false},
}


@misc{liu2025morechangelessanatomyaware,
      title={See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement}, 
      author={Junqi Liu and Zejun Wu and Pedro R. A. S. Bassi and Xinze Zhou and Wenxuan Li and Ibrahim E. Hamamci and Sezgin Er and Tianyu Lin and Yi Luo and Szymon PÅ‚otka and Bjoern Menze and Daguang Xu and Kai Ding and Kang Wang and Yang Yang and Yucheng Tang and Alan L. Yuille and Zongwei Zhou*},
      year={2025},
      eprint={2512.07251},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2512.07251}, 
      journal={arXiv preprint arXiv:2512.07251},
preview={liu2025morechangelessanatomyaware.jpg},
description={},
project={https://www.zongweiz.com/dataset},
paper={},
code={https://github.com/MrGiovanni/SMILE},
arxiv={2512.07251},
abstract={Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.},
selected={false},
}

@article{chen2022cafs,
  title={CAFS: An Attention-Based Co-Segmentation Semi-Supervised Method for Nasopharyngeal Carcinoma Segmentation},
  author={Chen, Yitong# and Han, Guanghui# and Lin, Tianyu and Liu, Xiujian*},
  journal={Sensors},
  volume={22},
  number={13},
  pages={5053},
  year={2022},
  publisher={MDPI},
preview={},
description={},
project={},
paper={},
code={},
arxiv={},
abstract={Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.},
selected={false},
}

@article{li2026early,
  title={Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography},
  author={Wenxuan Li# and Pedro R. A. S. Bassi# and Lizhou Wu# and Xinze Zhou# and Yuxuan Zhao# and Qi Chen# and Szymon Plotka# and Tianyu Lin and Zheren Zhu and Marisa Martin and Justin Caskey and Shanshan Jiang and Xiaoxi Chen and Jaroslaw B. Ä†wikla and Artur Sankowski and Yaping Wu and Sergio Decherchi and Andrea Cavalli and Chandana Lall and Cristian Tomasetti and Yaxing Guo and Xuan Yu and Yuqing Cai and Hualin Qiao and Jie Bao and Chenhan Hu and Ximing Wang and Arkadiusz Sitek and Kai Ding and Heng Li and Meiyun Wang* and Dexin Yu* and Guang Zhang* and Yang Yang* and Kang Wang* and Alan L. Yuille* and Zongwei Zhou*},
  journal={arXiv preprint arXiv:2601.22134},
  year={2026},
preview={},
description={},
project={https://www.zongweiz.com/dataset},
paper={},
code={},
arxiv={2601.22134},
abstract={Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with Artificial Intelligence). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P < 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.},
selected={false},
}

@ARTICLE{11367016,
  author={Li, Xinran and Shuai, Yi and Liu, Chen and Chen, Qi and Lin, Tianyu and Guo, Pengfei and Yang, Dong and Zhao, Can and Bassi, Pedro R. A. S. and Xu, Daguang and Wang, Kang and Yang, Yang and Yuille, Alan L. and Zhou, Zongwei*},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Text-Driven Tumor Synthesis}, 
  year={2026},
  volume={},
  number={},
  pages={1-1},
  keywords={Tumors;Computed tomography;Radiology;Artificial intelligence;Biomedical imaging;Three-dimensional displays;Training;Electronic mail;Diffusion models;Liver;Tumor Synthesis;Generative Models;Report Mining;Tumor Detection;Tumor Classification},
  doi={10.1109/TMI.2026.3658596},
preview={},
description={},
project={https://www.zongweiz.com/dataset},
paper={https://ieeexplore.ieee.org/document/11367016},
code={https://github.com/MrGiovanni/TextoMorph},
arxiv={2601.22134},
abstract={Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with Artificial Intelligence). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P < 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.},
selected={false},
}
